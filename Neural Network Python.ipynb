{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2c82e2",
   "metadata": {},
   "source": [
    " ### Tutorial Neural Network - PyTorch\n",
    " \n",
    " Neste notebook é apresentado algumas observações e formas de implementação e ajustes que podem ser feitos em uma rede neural! A explicação completa pode ser encontrada neste link, junto com o código. \n",
    " <link> https://machinelearningmastery.com/how-to-grid-search-hyperparameters-for-pytorch-models/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ae265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b0d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt(r'C:\\Users\\AnaLúciaLima/Documents/pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55b098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch classifier\n",
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(8, 12)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(12, 1)\n",
    "        self.prob = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.layer(x))\n",
    "        x = self.prob(self.output(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e535e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model with skorch\n",
    "model = NeuralNetClassifier(\n",
    "    PimaClassifier,\n",
    "    criterion=nn.BCELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054f189",
   "metadata": {},
   "source": [
    "##### Neste exemplo, temos o gridSearchCV (técnica de otimização de hiperparâmetros) para ajustar o número de:\n",
    "- batch_size é definido o número de amostras que serão propagadas pela rede antes de atualizar os parâmetros do modelo interno.\n",
    "- max_epochs é definido como um hiperparâmetro que define o número de vezes que o algoritmo de aprendizado funcionará em todo o conjunto de dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927d3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "    'max_epochs': [10, 50, 100]\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da1230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.742188 using {'batch_size': 20, 'max_epochs': 100}\n",
      "0.674479 (0.030978) with: {'batch_size': 10, 'max_epochs': 10}\n",
      "0.662760 (0.007366) with: {'batch_size': 10, 'max_epochs': 50}\n",
      "0.710938 (0.022097) with: {'batch_size': 10, 'max_epochs': 100}\n",
      "0.677083 (0.013279) with: {'batch_size': 20, 'max_epochs': 10}\n",
      "0.708333 (0.040133) with: {'batch_size': 20, 'max_epochs': 50}\n",
      "0.742188 (0.003189) with: {'batch_size': 20, 'max_epochs': 100}\n",
      "0.679688 (0.016573) with: {'batch_size': 40, 'max_epochs': 10}\n",
      "0.683594 (0.030425) with: {'batch_size': 40, 'max_epochs': 50}\n",
      "0.708333 (0.029463) with: {'batch_size': 40, 'max_epochs': 100}\n",
      "0.653646 (0.022402) with: {'batch_size': 60, 'max_epochs': 10}\n",
      "0.686198 (0.011201) with: {'batch_size': 60, 'max_epochs': 50}\n",
      "0.709635 (0.032734) with: {'batch_size': 60, 'max_epochs': 100}\n",
      "0.643229 (0.010253) with: {'batch_size': 80, 'max_epochs': 10}\n",
      "0.696615 (0.038450) with: {'batch_size': 80, 'max_epochs': 50}\n",
      "0.707031 (0.030758) with: {'batch_size': 80, 'max_epochs': 100}\n",
      "0.644531 (0.003189) with: {'batch_size': 100, 'max_epochs': 10}\n",
      "0.690104 (0.018688) with: {'batch_size': 100, 'max_epochs': 50}\n",
      "0.695312 (0.027251) with: {'batch_size': 100, 'max_epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103827dd",
   "metadata": {},
   "source": [
    "#### Neural Network possue o algoritmo de otimização de treinamento \n",
    "\n",
    "- Nesta grid_search é definido alguns algoritmos de otimização, o algoritmos que apresentar melhor resultado é usado para treinar a rede. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f693eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model with skorch\n",
    "model = NeuralNetClassifier(\n",
    "    PimaClassifier,\n",
    "    criterion=nn.BCELoss,\n",
    "    max_epochs=100,\n",
    "    batch_size=10,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940e5ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.718750 using {'optimizer': <class 'torch.optim.adamax.Adamax'>}\n",
      "0.645833 (0.003683) with: {'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "0.700521 (0.039623) with: {'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "0.686198 (0.014382) with: {'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "0.582031 (0.052698) with: {'optimizer': <class 'torch.optim.adadelta.Adadelta'>}\n",
      "0.713542 (0.032578) with: {'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "0.718750 (0.029232) with: {'optimizer': <class 'torch.optim.adamax.Adamax'>}\n",
      "0.710938 (0.045218) with: {'optimizer': <class 'torch.optim.nadam.NAdam'>}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'optimizer': [optim.SGD, optim.RMSprop, optim.Adagrad, optim.Adadelta,\n",
    "                  optim.Adam, optim.Adamax, optim.NAdam],\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    " \n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3146d",
   "metadata": {},
   "source": [
    "#### Ajuste da taxa de aprendizagem e momentum \n",
    "A taxa de aprendizagem é de suma importancia para encontrar o equilíbrio  ideal entre a velocidade de convergência e a precisão. Já o momentum é utilizado para encontrar o equilíbrio ideal entre a velocidade de convergência e a estabilidade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "483f05dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.667969 using {'optimizer__lr': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.667969 (0.014616) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.644531 (0.025315) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.2}\n",
      "0.647135 (0.009744) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.4}\n",
      "0.649740 (0.017566) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.6}\n",
      "0.664062 (0.008438) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.8}\n",
      "0.552083 (0.141826) with: {'optimizer__lr': 0.001, 'optimizer__momentum': 0.9}\n",
      "0.653646 (0.017566) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.648438 (0.005524) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.2}\n",
      "0.450521 (0.142719) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.4}\n",
      "0.649740 (0.003683) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.6}\n",
      "0.348958 (0.001841) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.8}\n",
      "0.450521 (0.142719) with: {'optimizer__lr': 0.01, 'optimizer__momentum': 0.9}\n",
      "0.549479 (0.142719) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.0}\n",
      "0.348958 (0.001841) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.2}\n",
      "0.348958 (0.001841) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.4}\n",
      "0.450521 (0.142719) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.6}\n",
      "0.549479 (0.142719) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.8}\n",
      "0.549479 (0.142719) with: {'optimizer__lr': 0.1, 'optimizer__momentum': 0.9}\n",
      "0.450521 (0.142719) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.0}\n",
      "0.450521 (0.142719) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.2}\n",
      "0.450521 (0.142719) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.4}\n",
      "0.549479 (0.142719) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.6}\n",
      "0.447917 (0.141790) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.8}\n",
      "0.450521 (0.142719) with: {'optimizer__lr': 0.2, 'optimizer__momentum': 0.9}\n",
      "0.348958 (0.001841) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.0}\n",
      "0.549479 (0.142719) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.2}\n",
      "0.447917 (0.141790) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.4}\n",
      "0.552083 (0.141790) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.6}\n",
      "0.552083 (0.141790) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.8}\n",
      "0.549479 (0.142719) with: {'optimizer__lr': 0.3, 'optimizer__momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'optimizer__lr': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'optimizer__momentum': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9],\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9743f",
   "metadata": {},
   "source": [
    "#### Ajuste da inicialização do peso da rede\n",
    "\n",
    "A inicialização do peso é um fator extremamente importante nas redes neurais, pois pode afetar diretamente a velocidade de convergência e o desempenho do modelo. Para que o treinamento seja eficaz é importante definir inicialmente os valores dos pesos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6282f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch classifier\n",
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self, weight_init=init.xavier_uniform_):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(8, 12)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(12, 1)\n",
    "        self.prob = nn.Sigmoid()\n",
    "        # manually init weights\n",
    "        weight_init(self.layer.weight)\n",
    "        weight_init(self.output.weight)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.layer(x))\n",
    "        x = self.prob(self.output(x))\n",
    "        return x\n",
    " \n",
    "# create model with skorch\n",
    "model = NeuralNetClassifier(\n",
    "    PimaClassifier,\n",
    "    criterion=nn.BCELoss,\n",
    "    optimizer=optim.Adamax,\n",
    "    max_epochs=100,\n",
    "    batch_size=10,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d4a22c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.705729 using {'module__weight_init': <function xavier_normal_ at 0x0000019FF39E0B80>}\n",
      "0.348958 (0.001841) with: {'module__weight_init': <function uniform_ at 0x0000019FF39E05E0>}\n",
      "0.555990 (0.136303) with: {'module__weight_init': <function normal_ at 0x0000019FF39E0670>}\n",
      "0.658854 (0.012075) with: {'module__weight_init': <function zeros_ at 0x0000019FF39E08B0>}\n",
      "0.705729 (0.024150) with: {'module__weight_init': <function xavier_normal_ at 0x0000019FF39E0B80>}\n",
      "0.571615 (0.156000) with: {'module__weight_init': <function xavier_uniform_ at 0x0000019FF39E0AF0>}\n",
      "0.678385 (0.018414) with: {'module__weight_init': <function kaiming_normal_ at 0x0000019FF39E0D30>}\n",
      "0.541667 (0.137640) with: {'module__weight_init': <function kaiming_uniform_ at 0x0000019FF39E0CA0>}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'module__weight_init': [init.uniform_, init.normal_, init.zeros_,\n",
    "                           init.xavier_normal_, init.xavier_uniform_,\n",
    "                           init.kaiming_normal_, init.kaiming_uniform_]\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    " \n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1999c",
   "metadata": {},
   "source": [
    "#### Ajuste da função de ativação de neurônio\n",
    "\n",
    "A função de ativação faz a transformação não-linear nos dados de entrada, tonando-o capaz de aprender e executar tarefas mais complexas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7dfe1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(8, 12)\n",
    "        self.act = activation()\n",
    "        self.output = nn.Linear(12, 1)\n",
    "        self.prob = nn.Sigmoid()\n",
    "        # manually init weights\n",
    "        init.kaiming_uniform_(self.layer.weight)\n",
    "        init.kaiming_uniform_(self.output.weight)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.layer(x))\n",
    "        x = self.prob(self.output(x))\n",
    "        return x\n",
    " \n",
    "# create model with skorch\n",
    "model = NeuralNetClassifier(\n",
    "    PimaClassifier,\n",
    "    criterion=nn.BCELoss,\n",
    "    optimizer=optim.Adamax,\n",
    "    max_epochs=100,\n",
    "    batch_size=10,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e053139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.694010 using {'module__activation': <class 'torch.nn.modules.activation.Softplus'>}\n",
      "0.350260 (0.003683) with: {'module__activation': <class 'torch.nn.modules.linear.Identity'>}\n",
      "0.691406 (0.019918) with: {'module__activation': <class 'torch.nn.modules.activation.ReLU'>}\n",
      "0.582031 (0.164712) with: {'module__activation': <class 'torch.nn.modules.activation.ELU'>}\n",
      "0.643229 (0.006639) with: {'module__activation': <class 'torch.nn.modules.activation.ReLU6'>}\n",
      "0.692708 (0.020505) with: {'module__activation': <class 'torch.nn.modules.activation.GELU'>}\n",
      "0.694010 (0.027866) with: {'module__activation': <class 'torch.nn.modules.activation.Softplus'>}\n",
      "0.653646 (0.019225) with: {'module__activation': <class 'torch.nn.modules.activation.Softsign'>}\n",
      "0.632812 (0.024910) with: {'module__activation': <class 'torch.nn.modules.activation.Tanh'>}\n",
      "0.656250 (0.028705) with: {'module__activation': <class 'torch.nn.modules.activation.Sigmoid'>}\n",
      "0.644531 (0.005524) with: {'module__activation': <class 'torch.nn.modules.activation.Hardsigmoid'>}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'module__activation': [nn.Identity, nn.ReLU, nn.ELU, nn.ReLU6,\n",
    "                           nn.GELU, nn.Softplus, nn.Softsign, nn.Tanh,\n",
    "                           nn.Sigmoid, nn.Hardsigmoid]\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    " \n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752ce44",
   "metadata": {},
   "source": [
    "#### Ajuste da regularização do Dropout\n",
    "\n",
    "O Dropout é uma técnica regularizadora em deep learning que ajuda a previnir o overfitting. O dropout age descartando aleatoriamente ou ignorando alguns neurônios durante o treinamento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fe72c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch classifier\n",
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5, weight_constraint=1.0):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(8, 12)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output = nn.Linear(12, 1)\n",
    "        self.prob = nn.Sigmoid()\n",
    "        self.weight_constraint = weight_constraint\n",
    "        # manually init weights\n",
    "        init.kaiming_uniform_(self.layer.weight)\n",
    "        init.kaiming_uniform_(self.output.weight)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # maxnorm weight before actual forward pass\n",
    "        with torch.no_grad():\n",
    "            norm = self.layer.weight.norm(2, dim=0, keepdim=True).clamp(min=self.weight_constraint / 2)\n",
    "            desired = torch.clamp(norm, max=self.weight_constraint)\n",
    "            self.layer.weight *= (desired / norm)\n",
    "        # actual forward pass\n",
    "        x = self.act(self.layer(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.prob(self.output(x))\n",
    "        return x\n",
    " \n",
    "# create model with skorch\n",
    "model = NeuralNetClassifier(\n",
    "    PimaClassifier,\n",
    "    criterion=nn.BCELoss,\n",
    "    optimizer=optim.Adamax,\n",
    "    max_epochs=100,\n",
    "    batch_size=10,\n",
    "    verbose=False\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f13821b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.717448 using {'module__dropout_rate': 0.1, 'module__weight_constraint': 1.0}\n",
      "0.597656 (0.178523) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 1.0}\n",
      "0.692708 (0.012075) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 2.0}\n",
      "0.578125 (0.162755) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 3.0}\n",
      "0.592448 (0.170935) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 4.0}\n",
      "0.683594 (0.022999) with: {'module__dropout_rate': 0.0, 'module__weight_constraint': 5.0}\n",
      "0.717448 (0.027126) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 1.0}\n",
      "0.707031 (0.031412) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 2.0}\n",
      "0.697917 (0.014382) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 3.0}\n",
      "0.705729 (0.009207) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 4.0}\n",
      "0.696615 (0.014382) with: {'module__dropout_rate': 0.1, 'module__weight_constraint': 5.0}\n",
      "0.688802 (0.023073) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 1.0}\n",
      "0.684896 (0.021710) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 2.0}\n",
      "0.584635 (0.165431) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 3.0}\n",
      "0.692708 (0.021236) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 4.0}\n",
      "0.675781 (0.019137) with: {'module__dropout_rate': 0.2, 'module__weight_constraint': 5.0}\n",
      "0.686198 (0.021236) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 1.0}\n",
      "0.665365 (0.019488) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 2.0}\n",
      "0.677083 (0.023939) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 3.0}\n",
      "0.660156 (0.011049) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 4.0}\n",
      "0.660156 (0.025315) with: {'module__dropout_rate': 0.3, 'module__weight_constraint': 5.0}\n",
      "0.670573 (0.023510) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 1.0}\n",
      "0.670573 (0.016367) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 2.0}\n",
      "0.667969 (0.011500) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 3.0}\n",
      "0.665365 (0.021710) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 4.0}\n",
      "0.684896 (0.027126) with: {'module__dropout_rate': 0.4, 'module__weight_constraint': 5.0}\n",
      "0.684896 (0.024150) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 1.0}\n",
      "0.670573 (0.014382) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 2.0}\n",
      "0.654948 (0.017566) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 3.0}\n",
      "0.576823 (0.159928) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 4.0}\n",
      "0.673177 (0.015073) with: {'module__dropout_rate': 0.5, 'module__weight_constraint': 5.0}\n",
      "0.664062 (0.020915) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 1.0}\n",
      "0.658854 (0.022402) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 2.0}\n",
      "0.667969 (0.014616) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 3.0}\n",
      "0.654948 (0.006639) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 4.0}\n",
      "0.569010 (0.155150) with: {'module__dropout_rate': 0.6, 'module__weight_constraint': 5.0}\n",
      "0.540365 (0.139256) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 1.0}\n",
      "0.550781 (0.140878) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 2.0}\n",
      "0.660156 (0.011049) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 3.0}\n",
      "0.580729 (0.093115) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 4.0}\n",
      "0.498698 (0.118067) with: {'module__dropout_rate': 0.7, 'module__weight_constraint': 5.0}\n",
      "0.645833 (0.009207) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 1.0}\n",
      "0.548177 (0.144561) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 2.0}\n",
      "0.446615 (0.139948) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 3.0}\n",
      "0.545573 (0.142791) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 4.0}\n",
      "0.407552 (0.076569) with: {'module__dropout_rate': 0.8, 'module__weight_constraint': 5.0}\n",
      "0.554688 (0.143666) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 1.0}\n",
      "0.606771 (0.068727) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 2.0}\n",
      "0.457031 (0.138438) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 3.0}\n",
      "0.453125 (0.138107) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 4.0}\n",
      "0.477865 (0.128267) with: {'module__dropout_rate': 0.9, 'module__weight_constraint': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'module__weight_constraint': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    'module__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    " \n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72244bb0",
   "metadata": {},
   "source": [
    "#### Ajustando o número de neurônios na camada oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc3a4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self, n_neurons=12):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(8, n_neurons)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.output = nn.Linear(n_neurons, 1)\n",
    "        self.prob = nn.Sigmoid()\n",
    "        self.weight_constraint = 2.0\n",
    "        # manually init weights\n",
    "        init.kaiming_uniform_(self.layer.weight)\n",
    "        init.kaiming_uniform_(self.output.weight)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # maxnorm weight before actual forward pass\n",
    "        with torch.no_grad():\n",
    "            norm = self.layer.weight.norm(2, dim=0, keepdim=True).clamp(min=self.weight_constraint / 2)\n",
    "            desired = torch.clamp(norm, max=self.weight_constraint)\n",
    "            self.layer.weight *= (desired / norm)\n",
    "        # actual forward pass\n",
    "        x = self.act(self.layer(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.prob(self.output(x))\n",
    "        return x\n",
    " \n",
    "# create model with skorch\n",
    "model = NeuralNetClassifier(\n",
    "    PimaClassifier,\n",
    "    criterion=nn.BCELoss,\n",
    "    optimizer=optim.Adamax,\n",
    "    max_epochs=100,\n",
    "    batch_size=10,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85bb4ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.712240 using {'module__n_neurons': 25}\n",
      "0.649740 (0.003683) with: {'module__n_neurons': 1}\n",
      "0.696615 (0.033502) with: {'module__n_neurons': 5}\n",
      "0.675781 (0.035943) with: {'module__n_neurons': 10}\n",
      "0.694010 (0.045592) with: {'module__n_neurons': 15}\n",
      "0.696615 (0.010253) with: {'module__n_neurons': 20}\n",
      "0.712240 (0.027126) with: {'module__n_neurons': 25}\n",
      "0.701823 (0.034104) with: {'module__n_neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'module__n_neurons': [1, 5, 10, 15, 20, 25, 30]\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    " \n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200cf20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cfa5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
